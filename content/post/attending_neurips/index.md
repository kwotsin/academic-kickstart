---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "NeurIPS 2019"
subtitle: ""
summary: "A short reflection on an amazing conference."
authors: []
tags: []
categories: []
date: 2020-01-06T16:36:08Z
lastmod: 2020-01-06T16:36:08Z
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

## Some Reflections
{{< figure src="images/poster.png" 
title="After presenting the work at the ITML workshop. Left to right: Trung, me, Prof Man."
numbered="false"
lightbox="true" >}}

In December 2019, I was fortunate enough to attend NeurIPS and present my work on InfoMax-GAN ([poster](https://drive.google.com/file/d/1Pxtp04gdxlTXk0IY_tp7j9dpQKmU7Cdh/view?usp=sharing), [paper](https://drive.google.com/file/d/13T8dBxFHYmypBhh2_r8KGkYTNmGcPvFP/view)) at the Information Theory and Machine Learning (ITML) workshop. The experience has been both inspiring and incredible. I've realised the greater depths of AI/ML than what I was previously exposed to, which is hugely motivating in wanting to learn more about the field. Before the conference, I probably would not have learnt about things like the generalisation bounds of neural networks, simply because I haven't worked in the area or even heard of the term. As if by diffusion, simply getting exposed to these new keywords and basic ideas alone was helpful in stitching discrete parts of the field together. It was an incredible intellectual exposure, particularly talking to researchers whose work I've been following a lot, and just having this larger picture of how research in this field is after all. 

I'm very grateful towards Trung and Prof Man for working with me on InfoMax-GAN, despite my inexperience in GAN research before this. This work was only possible thanks to their advice and the many nights of working through the intricacies of GANs, questioning exactly why things work and writing the discoveries into a work to hopefully benefit researchers. It has been a great learning experience!


## Music
Interestingly, one of the highlights of the conference for me was actually the music performances at the end of the week. I was amazed by Pablo Samuel Castro's ([@pcastr](https://twitter.com/pcastr?lang=en)) arrangement of Norwegian Wood, coupled with the amazing Tabla performance by Saurabh Kumar. I encourage you to check out his twitter page, which is full of creative works, including visualising music with GANs!


## Vancouver
Vancouver looks amazing. I had not been to North America before this, and it was my first time witnessing rolling fogs in the morning over the islands by the bay.

{{< figure src="images/panes.JPG" 
title="View from inside the convention centre."
numbered="false"
lightbox="true" >}}

{{< figure src="images/coast.png" 
title="Closer view of the coast."
numbered="false"
lightbox="true" >}}

{{< figure src="images/suspension.png" 
title="At Capilano Suspension Bridge."
numbered="false"
lightbox="true" >}}

## Reading List
A reading list I collated from interesting papers I found at the conference.

#### Generative Modeling
1. [Progressive Augmentation of GANs](https://arxiv.org/abs/1901.10422)
2. [Training Language GANs from Scratch](https://arxiv.org/abs/1905.09922)
3. [Twin Auxiliary Classifier GANs](https://arxiv.org/abs/1907.02690)
4. [Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)
5. [A Primal-Dual link between GANs and Autoencoders](https://papers.nips.cc/paper/8333-a-primal-dual-link-between-gans-and-autoencoders)
6. [Conditional Independence Testing using Generative Adversarial Networks](https://arxiv.org/abs/1907.04068)
7. [Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses](https://arxiv.org/abs/1902.03511)
8. [Invertible Convolutional Flow](https://papers.nips.cc/paper/8801-invertible-convolutional-flow)
9. [Implicit Generation and Generalization in Energy-Based Models](https://arxiv.org/abs/1903.08689)
10. [Emergence of Object Segmentation in Perturbed Generative Models](https://arxiv.org/abs/1905.12663)

#### GAN Evaluation Methods
1. [A Domain Agnostic Measure for Monitoring and Evaluating GANs](https://papers.nips.cc/paper/9377-a-domain-agnostic-measure-for-monitoring-and-evaluating-gans)
2. [Classification Accuracy Score for Conditional Generative Models](http://papers.nips.cc/paper/9393-classification-accuracy-score-for-conditional-generative-models)
3. [Improved Precision and Recall Metric for Assessing Generative Models](https://arxiv.org/abs/1904.06991)

#### GAN "Post-processing" Techniques
1. [Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting](https://arxiv.org/abs/1906.09531)
2. [Discriminator Optimal Transport](https://arxiv.org/abs/1910.06832)


#### Representation Learning
1. [Wasserstein Dependency Measure for Representation Learning](https://arxiv.org/abs/1903.11780)
2. [Adversarial Fisher Vectors for Unsupervised Representation Learning](https://arxiv.org/abs/1910.13101)
3. [Information Competing Process for Learning Diversified Representations](https://arxiv.org/abs/1906.01288)
4. [Putting An End to End-to-End: Gradient-Isolated Learning of Representations](https://arxiv.org/abs/1905.11786)

#### Information Theory
1. [Practical and Consistent Estimation of f-Divergences](https://arxiv.org/abs/1905.11112)
2. [GAIT: A Geometric Approach to Information Theory](https://arxiv.org/abs/1906.08325)
3. [Variational Predictive Information Bottleneck](https://arxiv.org/abs/1910.10831)

#### Interesting Concepts/Ideas
1. [Energy-Inspired Models: Learning with Sampler-Induced Distributions](https://arxiv.org/abs/1910.14265)
2. [Guided Similarity Separation for Image Retrieval](https://papers.nips.cc/paper/8434-guided-similarity-separation-for-image-retrieval)
3. [Einconv: Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks](https://papers.nips.cc/paper/8793-exploring-unexplored-tensor-network-decompositions-for-convolutional-neural-networks.pdf)
4. [Structural-Aware Sentence Similarity with Recursive Optimal Transport](https://arxiv.org/pdf/2002.00745.pdf)
5. [PRNet: Self-Supervised Learning for Partial-to-Partial Registration](https://arxiv.org/abs/1910.12240)
6. [Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations](https://arxiv.org/abs/1909.06628)
7. [Dancing to Music](https://arxiv.org/abs/1911.02001)
8. [Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks](https://arxiv.org/abs/1911.00937)
9. [Uniform convergence may be unable to explain generalization in deep learning](https://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning)

#### NeurIPS Tutorial References
A list based on the references for Arthur Gretton's insightful tutorial, found [here](https://slideslive.com/38923184/interpretable-comparison-of-distributions-and-models).

**Wasserstein Distances**
1. [Computational Optimal Transport](https://arxiv.org/abs/1803.00567)
2. [Optimal Transport for Applied Mathematicians](https://www.imo.universite-paris-saclay.fr/~filippo/OTAM-cvgmt.pdf)

**Maximum Mean Discrepancy (MMD)**
1. [A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)
2. [On Gradient Regularizers for MMD GANs](https://arxiv.org/abs/1805.11565)

**Variational Estimates of $\phi$-divergences**
1. [Estimating divergence functionals and the likelihood ratio by convex risk minimization](https://arxiv.org/abs/0809.0853)
2. [f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization](https://arxiv.org/abs/1606.00709)

**Divergences and Generative Models**
1. [Generalization and Equilibrium in Generative Adversarial Nets (GANs)](https://arxiv.org/abs/1703.00573)
2. [Wasserstein Autoencoders (2019 version)](https://arxiv.org/abs/1711.01558)
3. [Parametric Adversarial Divergences are Good Task Losses for Generative Modeling](https://arxiv.org/abs/1708.02511)
4. [Geometrical Insights for Implicit Generative Modeling](https://arxiv.org/abs/1712.07822)
